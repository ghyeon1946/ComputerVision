{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20190431_박규현(중간과제).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSPcxRc-8tNR"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==1.15"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Mount"
      ],
      "metadata": {
        "id": "v8tvs44Z9bO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "xnNe52Xk9Eqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "PO_vshob9fxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import _pickle as cPickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DATA_PATH = '/content/drive/MyDrive/4-1/컴퓨터비전/cifar-10-python/cifar-10-batches-py/'\n",
        "\n",
        "random.seed(123)\n",
        "tf.set_random_seed(123)\n",
        "np.random.seed(123)"
      ],
      "metadata": {
        "id": "vbIBIQiq9Esg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pickle 파일 불러오기\n",
        "def unpickle(file): \n",
        "    with open(os.path.join(DATA_PATH, file), 'rb') as fo:\n",
        "        dict = cPickle.load(fo, encoding='latin-1')\n",
        "\n",
        "    return dict\n",
        "\n",
        "\n",
        "# one-hot 인코딩\n",
        "def one_hot(labels, vals=10):\n",
        "    n = len(labels)\n",
        "    out = np.zeros((n, vals))\n",
        "    out[range(n), labels] = 1\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# cifar data 시각화\n",
        "def display_cifar(images, size):\n",
        "    n = len(images)\n",
        "    plt.figure()\n",
        "    plt.gca().set_axis_off()\n",
        "    im = np.vstack([np.hstack([images[np.random.choice(n)] for i in range(size)]) for i in range(size)])\n",
        "    #im = np.hstack([images[np.random.choice(n)] for i in range(size)])\n",
        "    #im = np.vstack([images[np.random.choice(n)] for i in range(size)])\n",
        "    plt.imshow(im)\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "# cifar 데이터로더 클래스\n",
        "class CifarLoader(object):\n",
        "    def __init__(self, source_files):\n",
        "        self._source = source_files\n",
        "        self._i = 0\n",
        "        self.images = None\n",
        "        self.labels = None\n",
        "\n",
        "    # 데이터 불러오기\n",
        "    def load(self):\n",
        "        data = [unpickle(f) for f in self._source]\n",
        "        images = np.vstack([d['data'] for d in data])\n",
        "        n = len(images)\n",
        "        self.images = images.reshape(n, 3, 32, 32).transpose(0, 2, 3, 1).astype(float)/255\n",
        "        self.labels = one_hot(np.hstack([d['labels'] for d in data]), 10)\n",
        "        \n",
        "        return self\n",
        "    \n",
        "\n",
        "    # 다음 배치\n",
        "    def next_batch(self, batch_size):\n",
        "        x = self.images[self._i: self._i+batch_size]\n",
        "        y = self.labels[self._i: self._i+batch_size]\n",
        "        self._i = (self._i + batch_size) % len(self.images)\n",
        "        return x, y\n",
        "    \n",
        "\n",
        "# cifar 데이터 매니저 클래스\n",
        "class CifarDataManager(object):\n",
        "    def __init__(self):\n",
        "        self.train = CifarLoader(['data_batch_{}'.format(i) for i in range(1, 6)]).load()\n",
        "        self.test = CifarLoader(['test_batch']).load()"
      ],
      "metadata": {
        "id": "fs1jKsl_9Euh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar = CifarDataManager()\n",
        "\n",
        "print('number of train images : {}'.format(len(cifar.train.images)))\n",
        "print('number of train labels : {}'.format(len(cifar.train.labels)))\n",
        "print('number of test images : {}'.format(len(cifar.test.images)))\n",
        "print('number of test labels : {}'.format(len(cifar.test.labels)))\n",
        "\n",
        "#images = cifar.train.images\n",
        "#display_cifar(images, 5)"
      ],
      "metadata": {
        "id": "Fn7VphXt9Ewo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BaseLine"
      ],
      "metadata": {
        "id": "t1hboMC-9LOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_weight = tf.initializers.truncated_normal(stddev=0.1)\n",
        "init_bias = tf.initializers.constant(0.1)\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
        "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "rate = tf.placeholder(tf.float32)\n",
        "\n",
        "# 32\n",
        "w1 = tf.Variable(init_weight(shape=[5,5,3,32], dtype=tf.float32))\n",
        "b1 = tf.Variable(init_bias(shape=[32]), dtype=tf.float32)\n",
        "conv1 = tf.nn.relu(tf.nn.conv2d(x, w1, strides=[1,1,1,1], padding='SAME')+b1)\n",
        "conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "\n",
        "# 64\n",
        "w2 = tf.Variable(init_weight(shape=[5,5,32,64], dtype=tf.float32))\n",
        "b2 = tf.Variable(init_bias(shape=[64]), dtype=tf.float32)\n",
        "conv2 = tf.nn.relu(tf.nn.conv2d(conv1_pool, w2, strides=[1,1,1,1], padding='SAME')+b2)\n",
        "conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "\n",
        "# flat\n",
        "conv2_flat = tf.reshape(conv2_pool, [-1, 8*8*64])\n",
        "\n",
        "# fully-connected: 1024\n",
        "w3 = tf.Variable(init_weight(shape=[8*8*64, 1024], dtype=tf.float32))\n",
        "b3 = tf.Variable(init_bias(shape=[1024]), dtype=tf.float32)\n",
        "full1 = tf.nn.relu(tf.matmul(conv2_flat, w3)+b3)\n",
        "full1_drop = tf.nn.dropout(full1, rate=rate) # drop-rate\n",
        "\n",
        "#fully-connected: 10\n",
        "w4 = tf.Variable(init_weight(shape=[1024, 10], dtype=tf.float32))\n",
        "b4 = tf.Variable(init_bias(shape=[10]), dtype=tf.float32)\n",
        "full2 = tf.matmul(full1_drop, w4)+b4\n",
        "\n",
        "logits = full2\n",
        "\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits))\n",
        "#train = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
        "train = tf.train.MomentumOptimizer(0.005, 0.9, use_nesterov=True).minimize(loss)\n",
        "\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(logits, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "metadata": {
        "id": "6Gf5jNgv9Eyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = cifar.test.images.reshape(10, 1000, 32, 32, 3)\n",
        "y_test = cifar.test.labels.reshape(10, 1000, 10)\n",
        "acc_data = []\n",
        "loss_data = []\n",
        "\n",
        "def test(sess):\n",
        "    t_acc = []\n",
        "    t_los = []\n",
        "    for i in range(10):\n",
        "        acc, los = sess.run([accuracy, loss], feed_dict={x: x_test[i], y: y_test[i], rate: 0.0})\n",
        "        t_acc.append(acc)\n",
        "        t_los.append(los)\n",
        "\n",
        "    acc = np.mean(t_acc)\n",
        "    los = np.mean(t_los)\n",
        "    acc_data.append(acc * 100)\n",
        "    loss_data.append(los)  \n",
        "\n",
        "    print('test accuracy: {:.4}%, test loss: {}'.format(acc*100, los))\n",
        "\n",
        "\n",
        "TRAIN_SIZE = 50000  \n",
        "BATCH_SIZE = 128\n",
        "STEPS = TRAIN_SIZE // BATCH_SIZE\n",
        "EPOCH = 50\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver(max_to_keep=10)\n",
        "    for i in range(EPOCH):\n",
        "        for j in range(STEPS):\n",
        "            batch = cifar.train.next_batch(BATCH_SIZE)\n",
        "            _, loss_, acc = sess.run([train, loss, accuracy], feed_dict={x: batch[0], y: batch[1], rate: 0.5})\n",
        "\n",
        "            if j%10 == 0:\n",
        "                print('epoch: {}, steps: {}, train-loss: {}, train-accuracy: {}'.format(i+1, j+1, loss_, acc))\n",
        "        # if (i+1) % 10 == 0:\n",
        "        #     saver.save(sess, DATA_PATH + f\"baseline/epoch{i+1}.ckpt\")\n",
        "        test(sess)\n",
        "\n",
        "sess.close()\n",
        "\n",
        "fig, ax = plt.subplots(2, constrained_layout=True)\n",
        "ax[0].plot(np.arange(1, EPOCH + 1), acc_data)\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[0].set_ylabel('Accuracy')\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].set_ylabel('Loss')\n",
        "ax[1].plot(np.arange(1, EPOCH + 1), loss_data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3c2CIqPq9E0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 최종 Model"
      ],
      "metadata": {
        "id": "bQtWeZaj9SMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_weight = tf.initializers.truncated_normal(stddev=0.1)\n",
        "init_bias = tf.initializers.constant(0.1)\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
        "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "rate = tf.placeholder(tf.float32)\n",
        "\n",
        "# 1F\n",
        "w1 = tf.Variable(init_weight(shape=[3,3,3,32], dtype=tf.float32))\n",
        "b1 = tf.Variable(init_bias(shape=[32]), dtype=tf.float32)\n",
        "conv1 = tf.nn.relu(tf.nn.conv2d(x, w1, strides=[1,1,1,1], padding='SAME')+b1)\n",
        "\n",
        "# 2F\n",
        "w2 = tf.Variable(init_weight(shape=[3,3,32,64], dtype=tf.float32))\n",
        "b2 = tf.Variable(init_bias(shape=[64]), dtype=tf.float32)\n",
        "conv2 = tf.nn.relu(tf.nn.conv2d(conv1, w2, strides=[1,1,1,1], padding='SAME')+b2)\n",
        "conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "\n",
        "# 3F\n",
        "w3 = tf.Variable(init_weight(shape=[3,3,64,128], dtype=tf.float32))\n",
        "b3 = tf.Variable(init_bias(shape=[128]), dtype=tf.float32)\n",
        "conv3 = tf.nn.relu(tf.nn.conv2d(conv2_pool, w3, strides=[1,1,1,1], padding='SAME')+b3)\n",
        "\n",
        "# 4F\n",
        "w4 = tf.Variable(init_weight(shape=[3,3,128,128], dtype=tf.float32))\n",
        "b4 = tf.Variable(init_bias(shape=[128]), dtype=tf.float32)\n",
        "conv4 = tf.nn.relu(tf.nn.conv2d(conv3, w4, strides=[1,1,1,1], padding='SAME')+b4)\n",
        "conv4_pool = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "\n",
        "# 5F\n",
        "w5 = tf.Variable(init_weight(shape=[3,3,128,256], dtype=tf.float32))\n",
        "b5 = tf.Variable(init_bias(shape=[256]), dtype=tf.float32)\n",
        "conv5 = tf.nn.relu(tf.nn.conv2d(conv4_pool, w5, strides=[1,1,1,1], padding='SAME')+b5)\n",
        "\n",
        "# 6F\n",
        "w6 = tf.Variable(init_weight(shape=[3,3, 256,256], dtype=tf.float32))\n",
        "b6 = tf.Variable(init_bias(shape=[256]), dtype=tf.float32)\n",
        "conv6 = tf.nn.relu(tf.nn.conv2d(conv5, w6, strides=[1,1,1,1], padding='SAME')+b6)\n",
        "conv6_pool = tf.nn.max_pool(conv6, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "\n",
        "# 7F flat\n",
        "conv7_flat = tf.reshape(conv6_pool, [-1, 4*4*256])\n",
        "\n",
        "# 8F fully-connected: 4*4*256\n",
        "w8 = tf.Variable(init_weight(shape=[4*4*256, 1024], dtype=tf.float32))\n",
        "b8 = tf.Variable(init_bias(shape=[1024]), dtype=tf.float32)\n",
        "full8 = tf.nn.relu(tf.matmul(conv7_flat, w8)+b8)\n",
        "full8_drop = tf.nn.dropout(full8, rate=rate) # drop-rate\n",
        "\n",
        "# 9F\n",
        "w9 = tf.Variable(init_weight(shape=[1024, 10], dtype=tf.float32))\n",
        "b9 = tf.Variable(init_bias(shape=[10]), dtype=tf.float32)\n",
        "full9 = tf.matmul(full8_drop, w9)+b9\n",
        "\n",
        "logits = full9\n",
        "\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits))\n",
        "#train = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
        "train = tf.train.MomentumOptimizer(0.001, 0.9, use_nesterov=True).minimize(loss)\n",
        "\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(logits, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "metadata": {
        "id": "sAcW-xVY9E3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = cifar.test.images.reshape(10, 1000, 32, 32, 3)\n",
        "y_test = cifar.test.labels.reshape(10, 1000, 10)\n",
        "acc_data = []\n",
        "loss_data = []\n",
        "\n",
        "loss_epoch=[]\n",
        "accuracy_epoch=[]\n",
        "\n",
        "def test(sess):\n",
        "    t_acc = []\n",
        "    t_los = []\n",
        "    for i in range(10):\n",
        "        acc, los = sess.run([accuracy, loss], feed_dict={x: x_test[i], y: y_test[i], rate: 0.0})\n",
        "        t_acc.append(acc)\n",
        "        t_los.append(los)\n",
        "\n",
        "    acc = np.mean(t_acc)\n",
        "    los = np.mean(t_los)\n",
        "    acc_data.append(acc * 100)\n",
        "    loss_data.append(los) \n",
        "\n",
        "    print('test accuracy: {:.4}%, test loss: {}'.format(acc*100, los))\n",
        "\n",
        "TRAIN_SIZE = 50000  \n",
        "BATCH_SIZE = 128\n",
        "STEPS = TRAIN_SIZE // BATCH_SIZE\n",
        "EPOCH = 150\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver(max_to_keep=10)\n",
        "    for i in range(EPOCH):\n",
        "        loss_batch=[]\n",
        "        accuracy_batch=[]\n",
        "\n",
        "        for j in range(STEPS):\n",
        "            batch = cifar.train.next_batch(BATCH_SIZE)\n",
        "            _, loss_, acc = sess.run([train, loss, accuracy], feed_dict={x: batch[0], y: batch[1], rate: 0.5})\n",
        "\n",
        "            if j%10 == 0:\n",
        "                print('epoch: {}, steps: {}, train-loss: {}, train-accuracy: {}'.format(i+1, j+1, loss_, acc))\n",
        "\n",
        "            loss_batch.append(loss_) \n",
        "            accuracy_batch.append(acc)\n",
        "\n",
        "        mean_loss = np.mean(loss_batch)\n",
        "        mean_accuracy = np.mean(accuracy_batch)    \n",
        "        loss_epoch.append(mean_loss)\n",
        "        accuracy_epoch.append(mean_accuracy * 100)\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            saver.save(sess, DATA_PATH + f\"mynet3/epoch{i+1}.ckpt\")\n",
        "\n",
        "        test(sess)\n",
        "\n",
        "sess.close()\n",
        "\n",
        "print('Best Train Acc : {:.2f}, Best Test Acc : {:.2f}'.format(max(accuracy_epoch), max(acc_data)))\n",
        "\n",
        "fig, ax = plt.subplots(2, constrained_layout=True)\n",
        "\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[0].set_ylabel('Accuracy')\n",
        "ax[0].plot(np.arange(1, EPOCH + 1), accuracy_epoch, 'r')\n",
        "ax[0].plot(np.arange(1, EPOCH + 1), acc_data, 'b')\n",
        "\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].set_ylabel('Loss')\n",
        "ax[1].plot(np.arange(1, EPOCH + 1), loss_epoch, 'r')\n",
        "ax[1].plot(np.arange(1, EPOCH + 1), loss_data, 'b')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pBAIcpGJ9E5M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}